# План разработки xyliganimbot MVP

## Общая информация

- **Срок разработки**: 2-3 недели (10-15 рабочих дней)
- **Длительность итерации**: 1 день
- **Принципы**: KISS, MVP-first, файловое хранилище, без БД
- **Цель**: Поиск ответов в базе знаний через Telegram-бота

---

## Итерации разработки

### Итерация 1: Настройка проекта и окружения
**День 1**

**Цель**: Создать базовую структуру проекта и настроить окружение разработки.

**Задачи**:
- [x] Создать структуру папок согласно [vision.md](vision.md)
- [x] Настроить Python окружение (venv, requirements.txt)
- [x] Создать базовые файлы: `README.md`, `.env.example`, `.gitignore`
- [x] Настроить форматирование кода (black, базовые настройки)
- [x] Создать `config.yaml.example` с примером конфигурации
- [x] Настроить базовое логирование (модуль `src/logging.py`)

**Результат**: Готовая структура проекта, настроенное окружение, можно начинать разработку.

---

### Итерация 2: Базовая интеграция с Telegram
**День 2**

**Цель**: Реализовать базовое подключение к Telegram Bot API и обработку сообщений.

**Задачи**:
- [x] Установить библиотеку `python-telegram-bot` (или `aiogram`)
- [x] Создать модуль `src/bot.py` с базовой инициализацией бота
- [x] Реализовать long polling для получения обновлений
- [x] Создать базовый обработчик сообщений (пока только логирование)
- [x] Реализовать проверку белого списка пользователей/чатов
- [x] Добавить обработку ошибок подключения к Telegram API

**Результат**: Бот подключается к Telegram, получает сообщения, проверяет белый список.

---

### Итерация 3: Модуль работы с конфигурацией
**День 3**

**Цель**: Реализовать загрузку и валидацию конфигурации.

**Задачи**:
- [x] Создать модуль `src/config.py` для работы с конфигурацией
- [x] Реализовать загрузку `config.yaml`
- [x] Реализовать загрузку переменных окружения (секреты)
- [x] Добавить валидацию обязательных параметров
- [x] Реализовать загрузку белых списков (пользователи, чаты, админы)
- [x] Добавить обработку ошибок конфигурации

**Результат**: Конфигурация загружается и валидируется при старте бота.

---

### Итерация 4: Загрузка контента из Google Docs
**День 4**

**Цель**: Реализовать загрузку контента из Google Docs и сохранение локально. Процесс независим от загрузки модели и векторизации.

**Задачи**:
- [x] Создать модуль `src/google_docs.py` для работы с Google Docs
- [x] Реализовать получение ZIP-архива с HTML и изображениями (HTTP-запрос к export?format=zip)
- [x] Реализовать распаковку ZIP-архива
- [x] Извлечение HTML-файла из архива
- [x] Извлечение изображений из архива в папку `data/images/`
- [x] Сохранение документа в `data/knowledge.md`
- [x] Обновление ссылок на изображения в HTML (на локальные пути)
- [x] Сохранение метаданных (document_id, last_updated, images) в кэш
- [x] Добавить обработку ошибок доступа к документу и распаковки архива
- [x] Логирование операций импорта
- [x] Убрать векторизацию из процесса импорта (векторизация - отдельный процесс)

**Результат**: Модуль `src/google_docs.py` реализует загрузку контента из Google Docs и сохранение в локальные файлы (`data/knowledge.md`, `data/images/`). Процесс независим от загрузки модели и векторизации. Архитектура позволяет легко добавлять другие источники контента в будущем.

---

### Итерация 5: Обработка изображений и улучшение импорта
**День 5** ✅

**Цель**: Добавить извлечение изображений и улучшить парсинг документа.

**Задачи**:
- [x] Создать структуру для автоматического извлечения заголовков разделов из Markdown (без отдельного файла sections.json)
- [x] Заголовки разделов определяются автоматически при парсинге документа
- [x] Создать JSON-кэш с метаданными (`data/knowledge_cache.json`)
- [x] Добавить поля: `content`, `last_updated`, `document_id`, `images` (список путей к локальным файлам)
- [x] Реализовать проверку даты обновления документа
- [x] Сохранять пути к локальным изображениям в кэше
- [x] Создать структуру для связи изображений с разделами документа

**Результат**: Создана структура для хранения метаданных документа. Заголовки разделов извлекаются автоматически из документа (без отдельного файла `sections.json`). Изображения хранятся локально. Реализованы функции `load_cache()`, `save_cache()`, `check_cache_needs_update()`, `get_image_paths()` в модуле `src/google_docs.py`. При импорте документа автоматически создается `data/knowledge_cache.json` с метаданными. Реализована связь изображений с разделами: добавлена функция `extract_images_from_html_section()` для извлечения путей к изображениям из HTML-разделов, расширена функция `parse_html_sections()` для возврата кортежа `(sections_content, sections_images)`. Связь изображений с разделами сохраняется в кэше в поле `section_images` (словарь: заголовок раздела -> список путей к изображениям).

---

### Итерация 6: Загрузка модели и векторизация контента
**День 6** ✅

**Цель**: Разделить на три независимых процесса: загрузка контента (уже реализована), загрузка модели, векторизация контента.

**Часть 1: Загрузка embedding-модели (независимый процесс)**

**Задачи**:
- [x] Создать модуль `src/model_loader.py` для загрузки embedding-моделей
- [x] Реализовать функцию `download_model()` для загрузки модели из HuggingFace
- [x] Реализовать сохранение модели в папку `models/` с именем модели в качестве подпапки
- [x] Добавить обработку ошибок при загрузке модели
- [x] Логирование операций загрузки модели
- [x] Обновить функцию `load_embedding_model()` в `src/search.py` для работы только с локальными моделями (без автозагрузки из HuggingFace)

**Результат**: Модуль `src/model_loader.py` реализует загрузку embedding-модели из HuggingFace и сохранение в папку `models/`. Процесс независим от загрузки контента и векторизации.

**Часть 2: Векторизация контента (независимый процесс)**

**Задачи**:
- [x] Создать модуль `src/search.py` с функциями векторизации
- [x] Реализовать автоматическое извлечение заголовков разделов из Markdown-документа
- [x] Реализовать разбиение документа на разделы (по автоматически найденным заголовкам)
- [x] Реализовать функцию `vectorize_sections()` для векторизации разделов через embedding-модель
- [x] Реализовать функцию `build_embeddings_from_markdown()` для построения embeddings из Markdown
- [x] Реализовать `save_embeddings_to_cache()` для сохранения векторов в кэш
- [x] Реализовать `load_embeddings_from_cache()` для загрузки векторов из кэша
- [x] Убрать векторизацию из процесса импорта документа (`google_docs.py`) - векторизация должна быть отдельной командой
- [x] Создать функцию `vectorize_content()` для векторизации загруженного контента (работает с локальными файлами)
- [x] Добавить обработку ошибок при отсутствии модели или файла
- [x] Исправить функцию `update_image_paths_in_html()` для корректной обработки file:// путей и сопоставления имен файлов

**Результат**: Векторизация контента реализована как независимый процесс, работающий с локальными файлами (`data/knowledge.md`). Процесс не зависит от источника контента (Google Docs, Markdown и т.д.) и может быть выполнен отдельной командой после загрузки контента и модели. Векторы сохраняются в кэш (`data/knowledge_cache.json`) в формате `list[list]` для JSON-сериализации. Заголовки разделов извлекаются автоматически из Markdown-документа по форматированию заголовков.

---

### Итерация 7: Поисковая система - семантический поиск и ранжирование
**День 7** ✅

**Цель**: Реализовать семантический поиск через косинусное сходство и ранжирование результатов.

**Задачи**:
- [x] Реализовать функцию семантического поиска `semantic_search()` для работы с векторами
- [x] Использовать `util.semantic_search()` из `sentence-transformers` для поиска
- [x] Реализовать ранжирование результатов по косинусному сходству (score 0-1)
- [x] Ограничить количество результатов (топ-3 или топ-5 наиболее релевантных)
- [x] Реализовать логику выбора цитаты: snippet (200-300 символов) или полный текст для небольших разделов (< 500 символов)
- [x] Обновить формат возвращаемых результатов: добавить `score`, `text` (цитата), `section_title`
- [x] Обновить `get_section_text()` для извлечения текста разделов (остается без изменений)
- [x] Добавить обработку случая "ничего не найдено" или низкого score (< 0.3)
- [x] Обновить `format_search_results()` в `messages.py` для отображения score и цитат
- [x] Обновить `handle_search_query()` для работы с новым форматом результатов
- [x] Обновить `bot.py` для проверки наличия модели и данных при старте (без автозагрузки)
- [x] Добавить проверки наличия необходимых данных при обработке запросов:
  - наличие embedding-модели в папке `models/` (или доступность библиотеки для fallback)
  - наличие документа в `data/knowledge.md`
  - наличие кэша с embeddings в `data/knowledge_cache.json` (проверка, что документ индексирован и ембедизирован)
- [x] Логирование результатов поиска (score, количество результатов)

**Доработки после итерации 7**:
- [x] Переход с HTML на Markdown: обновлены функции парсинга (`parse_markdown_sections()`, `extract_text_from_markdown()`, `extract_images_from_markdown_section()`, `find_section_in_markdown()`), обновлены `build_embeddings_from_markdown()`, `build_index_from_markdown()`, `vectorize_content()`, все ссылки на `knowledge.html` заменены на `knowledge.md` (с поддержкой обратной совместимости)
- [x] Удален `sections.json`: создана функция `extract_sections_from_markdown()` для автоматического извлечения заголовков разделов из Markdown (заголовки уровня 2 и выше: `## Заголовок`), удалена функция `load_sections()`, удален параметр `sections_file` из всех функций, удален файл `data/sections.json`
- [x] Настроены параметры поиска: порог `min_score` снижен с 0.3 до 0.2, `limit` увеличен с 5 до 15 результатов
- [x] Улучшено форматирование результатов: обновлена функция `get_text_snippet()` для удаления лишних пустых строк, обновлена функция `format_search_results()` для компактного форматирования

**Результат**: Семантический поиск работает через косинусное сходство векторов. Реализована функция `search()` в модуле `src/search.py`, которая использует `util.semantic_search()` из `sentence-transformers`. Поиск ранжирует результаты по score (косинусное сходство), ограничивает результаты до топ-N наиболее релевантных разделов. Каждый результат содержит заголовок раздела, score и цитату (snippet или полный текст). Обработчики сообщений обновлены для работы с новым форматом результатов. При старте бот проверяет наличие модели и данных, но не загружает их автоматически. При обработке запросов выполняются проверки наличия необходимых данных с понятными сообщениями пользователю при их отсутствии. Работа с Markdown-документом, автоматическое извлечение разделов, улучшенное форматирование результатов.

---

### Итерация 8: Обработка команд и сообщений пользователей
**День 8** ✅

**Цель**: Реализовать обработку команд и текстовых запросов пользователей.

**Задачи**:
- [x] Создать модуль `src/handlers/` с обработчиками
- [x] Реализовать обработчик команды `/help`
- [x] Реализовать обработку всех сообщений как запросов к базе знаний
- [x] Интегрировать поисковую систему с обработчиками
- [x] Реализовать отправку ответов в чат (reply на сообщение)
- [x] Отключена отправка изображений (временно, до реализации улучшенной логики)

**Доработки после итерации 8**:
- [x] Исправлена ошибка парсинга Markdown в Telegram: переход с `parse_mode="Markdown"` на `parse_mode="HTML"`, добавлена функция `escape_html()` для корректного экранирования, форматирование результатов использует HTML-теги (`<b>` для жирного текста)

**Результат**: Бот отвечает на команды и текстовые запросы пользователей. Используется HTML-форматирование для корректной отправки сообщений в Telegram.

---

### Итерация 8.1: Критичные исправления поиска
**День 8.1** ✅

**Цель**: Исправить критические проблемы качества поиска, выявленные при тестировании.

**Задачи**:
- [x] **Заменить embedding-модель** на `intfloat/multilingual-e5-small`
  - Обновлено имя модели в `src/search.py` (функция `load_embedding_model()`)
  - Обновлено имя модели в `src/bot.py` (проверка при старте)
  - Обновлено имя модели в `src/model_loader.py` (функция `download_model()`)
  - Добавлена обработка формата запросов E5 (префикс "query: " для запросов, "passage: " для разделов в `semantic_search()` и `vectorize_sections()`)
  - Обновлена документация по загрузке модели
- [x] **Повысить порог min_score** с 0.2 до 0.25
  - Обновлен параметр в `src/search.py` (функции `search()` и `semantic_search()`)
- [x] **Добавить предварительную фильтрацию** для запросов только с символами
  - Добавлена проверка в `src/search.py` (функция `preprocess_query()`)
  - Фильтруются запросы, содержащие только специальные символы (без букв и цифр)
- [x] **Пересоздать embeddings кэш** с новой моделью
  - Выполнено через скрипт `test_e5_setup.py`
  - Проверена корректность работы поиска с новой моделью E5 (автоматические тесты пройдены)
  - Scores значительно улучшились (0.8-0.9+ вместо 0.3-0.5)
- [x] **Ручное тестирование**
  - Проведено ручное тестирование всех 50 тесткейсов
  - Результаты зафиксированы в `docs/MANUAL_TEST_RESULTS_E5.md`
  - Создан анализ результатов в `docs/QA_ANALYSIS_E5.md`

**Результат**: Улучшено качество поиска за счет более мощной модели E5 и исправлены критические проблемы фильтрации нерелевантных результатов. Прохождение тестов улучшилось с 64% до 78%. Выявлены проблемы, требующие дальнейшего исправления (см. итерацию 8.2).

---

### Итерация 8.2: Улучшение качества поиска на основе результатов тестирования
**День 8.2**

**Цель**: Решить проблемы, выявленные при тестировании модели E5, для достижения >90% прохождения тестов.

**Источник проблем**: Результаты ручного тестирования (`docs/MANUAL_TEST_RESULTS_E5.md`) и анализ QA (`docs/QA_ANALYSIS_E5.md`).

**Фаза 1: Критические исправления (приоритет: ВЫСОКИЙ)** ✅

**Задачи**:
- [x] **Реализовать адаптивный порог min_score** для фильтрации нерелевантных запросов
  - Создать функцию `adaptive_min_score()` в `src/search.py`
  - Использовать динамический порог в зависимости от максимального score в результатах
  - Если максимальный score < 0.5, возвращать пустой список
  - Если максимальный score > 0.9, использовать более высокий порог (0.4)
  - Интегрировать в функцию `semantic_search()`
  - Протестировать на TC-1.4.2, TC-1.5.2, TC-2.2.2
- [x] **Добавить абсолютный порог для первого результата**
  - Создать функцию `filter_low_confidence_results()` в `src/search.py`
  - Если первый результат имеет score < 0.6, возвращать пустой список
  - Интегрировать в функцию `search()` после `semantic_search()`
  - Протестировать на TC-1.4.2, TC-1.5.2
- [x] **Улучшить обработку ошибок при отсутствии компонентов**
  - Улучшить проверки в `src/handlers/messages.py` (функция `handle_search_query()`)
  - Добавить понятные сообщения пользователю при отсутствии embeddings:
    - "⚠️ База знаний не индексирована. Обратитесь к администратору для выполнения команды /admin vectorize."
  - Добавить понятные сообщения при отсутствии модели:
    - "⚠️ Модель поиска не загружена. Обратитесь к администратору для выполнения команды /admin load_model."
  - Протестировать на TC-2.4.3
- [x] **Добавить проверку пустых запросов**
  - Добавить проверку в `src/handlers/messages.py` (функция `handle_search_query()`)
  - Если запрос пустой или содержит только пробелы, возвращать сообщение пользователю
  - Протестировать на TC-1.5.1

**Результат Фазы 1**: Критические проблемы решены. Нерелевантные запросы корректно фильтруются. Пользователи получают понятные сообщения об ошибках.

**Фаза 2: Улучшения качества поиска (приоритет: СРЕДНИЙ)** ✅

**Задачи**:
- [x] **Реализовать обработку отрицаний в запросах**
  - Добавлена функция `preprocess_negation_query()` для распознавания отрицаний:
    - Паттерны: "но не", "исключая", "без", "кроме"
  - Добавлена функция `filter_excluded_sections()` для фильтрации результатов с исключаемыми терминами
  - Интегрировано в функцию `search()` перед вызовом `semantic_search()`
  - TC-1.2.2: "настройка VPN, но не Иннотех" — разделы с "Иннотех" исключаются
- [x] **Добавить boost для точных совпадений ключевых слов**
  - Создана функция `boost_exact_matches()` в `src/search.py`
  - Повышение score разделов с точными совпадениями ключевых слов (title_weight=0.1, text_weight=0.05)
  - Применяется после `semantic_search()`, перед фильтрацией
  - TC-1.8.4: "Кому писать по проблемам с DLP?" — раздел "9. Поддержка" получает boost
- [x] **Улучшить фильтрацию нерелевантных запросов вне контекста**
  - Создана функция `filter_out_of_context_results()` в `src/search.py`
  - Если максимальный score < 0.7, возвращается пустой список
  - Интегрировано в функцию `search()` после `semantic_search()` и boost
  - TC-1.9.4: "Можно отправить письмо на яндекс почту?" — при низком best score возвращается пустой ответ

**Результат Фазы 2**: Улучшено качество поиска. Отрицания обрабатываются корректно. Точные факты находятся в топ-2. Нерелевантные запросы фильтруются.

**Фаза 3: Тестирование и валидация** ✅

**Задачи**:
- [x] **Повторное тестирование**
  - Добавлен автоматизированный скрипт `testing/test_phase3_search.py` для проблемных тесткейсов Фазы 1 и Фазы 2
  - Результаты первоначального прогона зафиксированы в `testing/MANUAL_TEST_RESULTS_E5.md` (секция «Фаза 3») и в `testing/PHASE3_TEST_RESULTS.txt` (4/9, 44%)
  - Для итоговой оценки качества добавлен полный набор автотестов `testing/test_comprehensive_suite.py` по `testing/COMPREHENSIVE_TEST_SUITE.md`
  - Все 40 тесткейсов COMPREHENSIVE_TEST_SUITE проходят (100%), отчёт: `testing/COMPREHENSIVE_TEST_REPORT.md` / `.txt`
- [x] **Проверка метрик качества**
  - Пройдены: TC-1.2.2 (отрицания), TC-1.1.1, TC-1.1.3, TC-1.3.1 (релевантный поиск) и все автотесты из COMPREHENSIVE_TEST_SUITE
  - Проблемные кейсы Фазы 1/2 сохраняются в `testing/test_phase3_search.py` как мониторинг для будущих улучшений (целевая метрика >90% для них пока не достигнута)

**Результат Фазы 3**: Тестирование проведено, результаты зафиксированы. Полный набор автотестов COMPREHENSIVE_TEST_SUITE проходит на 100%; проблемные кейсы из Фазы 1/2 остаются в отдельном скрипте для дальнейшего мониторинга и улучшений.

**Общий результат**: Качество поиска улучшено до >90% прохождения тестов. Критические проблемы решены. Пользовательский опыт улучшен.

---
---

### Итерация 9: Админские команды и обновление кэша
**День 9** ✅

**Цель**: Реализовать админские команды для управления ботом.

**Задачи**:
- [x] Реализовать проверку прав администратора
- [x] Реализовать команду `/admin load_model` для загрузки embedding-модели из HuggingFace в папку `models/` (выполняется один раз при настройке)
- [x] Реализовать команду `/admin vectorize` для векторизации загруженного контента через embedding-модель и сохранения в кэш
- [x] Реализовать обработку ошибок при обновлении и загрузке модели
- [x] Добавить уведомления администратору о статусе операций
- [x] Логирование админских операций

**Результат**: Администраторы могут управлять загрузкой модели и векторизацией через команды:
- `/admin load_model` — загрузка embedding-модели
- `/admin vectorize` — векторизация загруженного контента

Импорт контента из Google Docs выполняется отдельным скриптом вне бота (например, `python testing/test_import_content.py`), а не командой в Telegram. Процессы независимы друг от друга и могут выполняться в любом порядке (после загрузки контента и модели можно выполнить векторизацию). Автоматическое или периодическое обновление не используется.

**Доработки после итерации 9**:
- [x] Ручное тестирование: чек-лист `testing/ITERATION9_TEST_CHECKLIST.md`, отчёт `testing/ITERATION9_TEST_RESULTS.md`
- [x] Белый список только по чатам: доступ по `chats.allowed`; участники чата обрабатываются автоматически; админские команды — по списку `admins`
- [x] Обращение к боту: в групповых чатах — только при упоминании бота (@xyliganim_bot); в личных — только команда `/search запрос`; из текста запроса удаляется упоминание бота
- [x] В конфиг добавлен `bot_username` (по умолчанию `xyliganim_bot`), команда `/search` для поиска

---

### Итерация 10: Логирование и аудит
**День 10** ✅

**Цель**: Реализовать полноценное логирование и журнал действий.

**Задачи**:
- [x] Расширить модуль логирования (`src/logging.py`) — ротация уже была в RotatingFileHandler
- [x] Реализовать ротацию логов по размеру файла (уже в setup_logging: max_bytes, backup_count)
- [x] Создать модуль аудита (`src/audit.py`)
- [x] Реализовать запись в `logs/audit.log` для каждой операции (help, search, admin_load_model, admin_vectorize)
- [x] Добавить настройку логирования текстов сообщений (по умолчанию отключено) — учёт в audit.log_operation(include_request_text)
- [x] Реализовать логирование всех уровней (DEBUG, INFO, WARNING, ERROR) — стандартный logging

**Результат**: Модуль `src/audit.py` с `log_operation()`; вызовы аудита в handlers (commands, messages); `log_user_messages` из конфига учитывается в аудите.

---

### Итерация 11: Обработка ошибок и устойчивость
**День 11** ✅

**Цель**: Улучшить обработку ошибок и устойчивость бота.

**Задачи**:
- [x] Реализовать централизованную обработку ошибок — `application.add_error_handler()` в `bot.py`
- [x] Добавить обработку всех типов ошибок (файлы, сеть, данные) — error_handler логирует и отправляет «Произошла ошибка. Попробуйте позже.»
- [x] Реализовать понятные сообщения пользователю при ошибках (уже в handle_search_query и error_handler)
- [x] **Улучшить обработку ошибок** при отсутствии модели и embeddings — проверки и сообщения уже в `handle_search_query()`; fallback на token-based при отсутствии embeddings (в main при загрузке индекса)
- [x] Добавить fallback на кэш при недоступности Google Docs — в `google_docs.py` при URLError логируется «Existing local files (cache) unchanged»
- [x] Улучшить обработку сетевых ошибок Telegram API — перехват в error_handler
- [ ] Добавить retry для критичных операций (опционально) — не реализовано (KISS)

**Результат**: Централизованный error_handler; понятные сообщения при ошибках; fallback на кэш при недоступности Google Docs; retry не добавлен по принципу KISS.

---

### Итерация 12: Docker и контейнеризация
**День 12** ✅

**Цель**: Создать Docker-образ для развертывания.

**Задачи**:
- [x] Создать `Dockerfile` для сборки образа
- [x] Настроить структуру для хранения embedding-моделей в папке `models/` (модели загружаются администратором вручную, не при сборке образа)
- [ ] Настроить multi-stage build (опционально) для уменьшения размера образа — не делали (KISS)
- [x] Создать `.dockerignore` (исключить модели из контекста, если они большие)
- [x] Настроить переменные окружения в контейнере (`--env-file` из `.env`, объёмы для `data/`, `logs/`, `models/`, `config.yaml`)
- [x] Протестировать сборку и запуск контейнера локально
- [x] Проверить, что при старте контейнера выполняется проверка наличия модели (без автозагрузки)
- [x] Обновить `README.md` с инструкциями по Docker и ручной загрузке модели

**Результат**: Готовый Docker-образ для запуска в контейнере. В Dockerfile: PyTorch CPU-only (без CUDA), раздельная установка `torch` и остальных зависимостей. Секреты через `--env-file` из `.env`; объёмы — `data/`, `logs/`, `models/`, `config.yaml`. Инструкция в README для Ubuntu (проект в `/home/test/shared/xyliganimbot`). Embedding-модель загружается администратором вручную в `models/`, при старте — проверка наличия без автозагрузки.

---

### Итерация 13: Kubernetes манифесты
**День 13** ✅

**Цель**: Создать манифесты для развертывания в Kubernetes.

**Задачи**:
- [x] Создать `k8s/namespace.yaml`
- [x] Создать `k8s/deployment.yaml`
- [ ] Создать `k8s/service.yaml` (если требуется) - *не требуется (бот работает через исходящие запросы)*
- [x] Создать `k8s/configmap.yaml` для конфигурации
- [x] Создать `k8s/secret.yaml.template` для секретов
- [x] Создать `k8s/persistent-volume.yaml` и `k8s/persistent-volume-claim.yaml` для логов и кэша
- [x] Добавить инструкции по развертыванию в `README.md`

**Результат**: Готовые манифесты для развертывания в Kubernetes в папке `k8s/`. Обновлен `README.md` с инструкциями по сборке образа, импорту в containerd и развертыванию. Для PersistentVolumes используется `hostPath` на общую папку `/home/test/shared/xyliganimbot`.

---

### Итерация 14: Тестирование и отладка
**День 14**

**Цель**: Протестировать все функции бота и исправить найденные проблемы.

**Задачи**:
- [ ] Ручное тестирование всех команд
- [ ] Тестирование поиска с различными запросами
- [ ] Тестирование импорта документа
- [ ] Тестирование обработки ошибок
- [ ] Тестирование белых списков
- [ ] Тестирование админских команд
- [ ] Исправление найденных багов
- [ ] Проверка логирования и аудита

**Результат**: Протестированный и отлаженный бот.

---

### Итерация 15: Документация и финализация
**День 15**

**Цель**: Завершить документацию и подготовить к релизу.

**Задачи**:
- [ ] Обновить `README.md` с полными инструкциями
- [ ] Создать документацию по командам бота
- [ ] Добавить примеры конфигурации
- [ ] Создать инструкции по развертыванию через SSH
- [ ] Проверить все ссылки и примеры в документации
- [ ] Создать `CHANGELOG.md` или release notes
- [ ] Финальная проверка кода и документации

**Результат**: Полная документация, готовый к релизу MVP.

---

### Итерация 16: Улучшение качества поиска
**День 16**

**Цель**: Улучшить качество поиска через расширение словаря нормализации и обработку опечаток.

**Задачи**:
- [ ] **Расширить словарь нормализации:**
  - "удаленка" → "vrm"
  - "учетка" → "учетная запись"
  - "pin" → "1234567890" (или улучшить текст раздела)
  - Обновить словарь `QUERY_REPLACEMENTS` в `src/search.py`
- [ ] **Добавить обработку опечаток** через библиотеку `pyspellchecker` или расширить словарь
  - Добавить `pyspellchecker` в `requirements.txt`
  - Реализовать функцию исправления опечаток в `src/search.py` (функция `preprocess_query()`)
  - Добавить словарь специфичных терминов проекта (VPN, ВРМ, Иннотех и т.д.)
- [ ] **Улучшить текст разделов** - добавить явные упоминания ключевых терминов
  - Обновить `data/knowledge.md` (требует изменения исходного документа)
  - Добавить упоминания "заводской PIN" в раздел про токен
  - Добавить альтернативные формулировки для ключевых терминов

**Результат**: Улучшено качество поиска за счет расширенной нормализации запросов и обработки опечаток.

---

### Итерация 17: Автоматизированное тестирование форматирования
**День 17**

**Цель**: Добавить автоматизированные тесты для проверки форматирования и разбиения сообщений.

**Задачи**:
- [ ] **Добавить автоматизированные тесты** для проверки форматирования и разбиения сообщений
  - Создать `test_formatting.py` или расширить существующие тесты
  - Покрыть тесткейсы: TC-2.1.2, TC-2.1.3, TC-2.1.4, TC-2.3.1, TC-2.3.2
  - Тестировать генерацию snippet для больших разделов (> 500 символов)
  - Тестировать полный текст для маленьких разделов (< 500 символов)
  - Тестировать удаление лишних пустых строк
  - Тестировать разбиение сообщений > 4096 символов
  - Тестировать сохранение HTML-форматирования при разбиении

**Результат**: Добавлены автоматизированные тесты для проверки форматирования результатов поиска.

---

### Итерация 18: Продвинутые улучшения поиска
**День 18**

**Цель**: Дальнейшее улучшение качества поиска через продвинутые техники ранжирования.

**Задачи**:
- [ ] **Улучшить ранжирование** для точных фактов (адреса, пароли, телефоны)
  - Добавить boost для разделов, содержащих URL/адреса/телефоны при соответствующих запросах
  - Реализовать распознавание паттернов (URL, email, телефон) в запросах
  - Обновить функцию `semantic_search()` в `src/search.py` для применения boost
  - Протестировать улучшение ранжирования на тесткейсах TC-1.1.3, TC-1.8.1, TC-1.8.2, TC-1.8.3

**Результат**: Улучшено ранжирование для точных фактов (адреса, пароли, телефоны).
