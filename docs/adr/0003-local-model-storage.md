# ADR-0003: Локальное хранение embedding-моделей

## Статус
Принято

## Контекст
Проект должен работать автономно в Docker-контейнере в РФ-сегменте, где доступ к HuggingFace может быть ограничен или замедлен. Модели необходимо загружать один раз и хранить локально.

## Решение
Хранить embedding-модели локально в папке `models/` проекта.

### Механизм работы
1. **При сборке Docker-образа**: модели скачиваются один раз и копируются в образ
2. **При первом запуске**: если модели нет локально, скачивается из HuggingFace (если доступен интернет)
3. **При последующих запусках**: модель загружается из локальной папки `models/`

### Структура
```
xyliganimbot/
├── models/
│   └── paraphrase-multilingual-MiniLM-L12-v2/  # или другая модель
│       ├── config.json
│       ├── pytorch_model.bin
│       └── ...
```

### В коде
```python
# Указываем локальный путь вместо облачного
model_path = "./models/paraphrase-multilingual-MiniLM-L12-v2"
# или
model_path = "paraphrase-multilingual-MiniLM-L12-v2"  # если в стандартной папке
```

## Последствия

### Положительные
- ✅ Полная автономность после первой загрузки
- ✅ Быстрая загрузка модели (из локальной папки)
- ✅ Работа без интернета в продакшене
- ✅ Предсказуемость (одна и та же версия модели)

### Отрицательные
- ❌ Увеличение размера Docker-образа (~500 МБ)
- ❌ Необходимость обновления образа при смене модели

### Нейтральные
- Модели можно версионировать через git LFS или хранить отдельно
- При разработке можно скачивать модели автоматически

## Альтернативы

### Отклонено: Скачивание модели при каждом запуске
- **Почему**: не работает в автономном режиме, медленнее, зависимость от интернета

### Отклонено: Использование внешнего хранилища моделей
- **Почему**: добавляет сложность, требует настройки доступа

## Связанные решения
- [ADR-0001: Использование семантического поиска](./0001-semantic-search-via-embeddings.md)
- [ADR-0002: Выбор библиотеки и модели](./0002-embedding-library-and-model.md)

## Дата
2026-01-16
