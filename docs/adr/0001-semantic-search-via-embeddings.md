# ADR-0001: Использование семантического поиска через embedding-модели

## Статус
Принято

## Контекст
Проект xyliganimbot предназначен для поиска ответов в базе знаний через Telegram-бота. Изначально планировался простой текстовый поиск через обратный индекс (token-based search), но анализ требований показал необходимость более качественного поиска:

- **Small Data**: небольшой объем данных (20-30 разделов, ~6000 слов)
- **Высокие требования к качеству**: семантический поиск, работа с IT-терминами, смешение языков (русский/английский)
- **Автономность**: работа в Docker, РФ-сегмент (ограниченный доступ к интернету)

## Решение
Использовать **семантический поиск через embedding-модели** вместо простого текстового поиска по ключевым словам.

### Технологический стек
- **Библиотека**: `sentence-transformers` (обертка над PyTorch/HuggingFace)
- **Модель**: `paraphrase-multilingual-MiniLM-L12-v2` или `multilingual-e5-small`
- **Хранение**: векторы в оперативной памяти (numpy array или PyTorch tensor)

### Алгоритм работы
1. **При импорте/старте**: все разделы векторизуются один раз через embedding-модель
2. **При запросе**: запрос пользователя векторизуется, вычисляется косинусное сходство со всеми разделами
3. **Ранжирование**: результаты сортируются по убыванию сходства (score 0-1)
4. **Возврат**: топ-N наиболее релевантных разделов с цитатами

## Последствия

### Положительные
- ✅ **Семантическое понимание**: поиск по смыслу, а не только по ключевым словам
- ✅ **Работа с синонимами**: "упал сервер" и "server down" будут найдены как релевантные
- ✅ **Мультиязычность**: поддержка русского и английского языков
- ✅ **Производительность**: для небольшого объема данных поиск выполняется за миллисекунды
- ✅ **Простота**: весь индекс в памяти, без необходимости в БД
- ✅ **Автономность**: модели хранятся локально, работа без интернета

### Отрицательные
- ❌ **Дополнительная зависимость**: требуется `sentence-transformers` и PyTorch
- ❌ **Размер моделей**: ~400-500 МБ для модели
- ❌ **Память**: ~10-20 МБ в оперативной памяти для модели
- ❌ **Время инициализации**: ~1-2 секунды при первом запуске для загрузки модели

### Нейтральные
- Модели хранятся локально в папке `models/` для автономной работы
- Векторы могут быть опционально сериализованы в кэш для ускорения загрузки

## Альтернативы

### Отклонено: Простой текстовый поиск (обратный индекс)
- **Почему**: не обеспечивает семантическое понимание, плохо работает с синонимами и смешением языков
- **Когда могло бы подойти**: если бы требовался только точный поиск по ключевым словам

### Отклонено: Векторные БД (ChromaDB, Pinecone, FAISS)
- **Почему**: избыточно для Small Data (20-30 разделов), добавляет сложность без реальной пользы
- **Когда могло бы подойти**: если бы объем данных был значительно больше (тысячи разделов)

### Отклонено: LLM для поиска
- **Почему**: избыточно для поиска, требует больше ресурсов, медленнее
- **Когда могло бы подойти**: для генерации ответов на основе найденной информации (планируется в будущем)

## Связанные решения
- [ADR-0002: Выбор библиотеки и модели для embedding](./0002-embedding-library-and-model.md)
- [ADR-0003: Локальное хранение моделей](./0003-local-model-storage.md)

## Дата
2026-01-16
